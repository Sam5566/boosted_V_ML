

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
New Run
Current date and time = 2022-09-20 17:23:29.207946
Binary task for  ['$W^+$', '$W^-$']
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
The data set contains images
The data set contains images
Epoch 1/500
375/375 [==============================] - 179s 452ms/step - loss: 5.0946 - accuracy: 0.7677 - val_loss: 1.7968 - val_accuracy: 0.8388

Epoch 00001: val_loss improved from inf to 1.79684, saving model to /home/samhuang/ML/best_model/best_model_binary-WpWm_CNN_kappa0.15_E/
Epoch 2/500
375/375 [==============================] - 26s 70ms/step - loss: 1.0616 - accuracy: 0.8501 - val_loss: 0.6469 - val_accuracy: 0.8491

Epoch 00002: val_loss improved from 1.79684 to 0.64693, saving model to /home/samhuang/ML/best_model/best_model_binary-WpWm_CNN_kappa0.15_E/
Epoch 3/500
375/375 [==============================] - 26s 68ms/step - loss: 0.5044 - accuracy: 0.8537 - val_loss: 0.4283 - val_accuracy: 0.8510

Epoch 00003: val_loss improved from 0.64693 to 0.42826, saving model to /home/samhuang/ML/best_model/best_model_binary-WpWm_CNN_kappa0.15_E/
Epoch 4/500
375/375 [==============================] - 26s 68ms/step - loss: 0.4001 - accuracy: 0.8547 - val_loss: 0.3888 - val_accuracy: 0.8519

Epoch 00004: val_loss improved from 0.42826 to 0.38878, saving model to /home/samhuang/ML/best_model/best_model_binary-WpWm_CNN_kappa0.15_E/
Epoch 5/500
375/375 [==============================] - 26s 69ms/step - loss: 0.3782 - accuracy: 0.8565 - val_loss: 0.3826 - val_accuracy: 0.8514

Epoch 00005: val_loss improved from 0.38878 to 0.38260, saving model to /home/samhuang/ML/best_model/best_model_binary-WpWm_CNN_kappa0.15_E/
Epoch 6/500
375/375 [==============================] - 26s 70ms/step - loss: 0.3713 - accuracy: 0.8576 - val_loss: 0.3753 - val_accuracy: 0.8527

Epoch 00006: val_loss improved from 0.38260 to 0.37533, saving model to /home/samhuang/ML/best_model/best_model_binary-WpWm_CNN_kappa0.15_E/
Epoch 7/500
375/375 [==============================] - 26s 69ms/step - loss: 0.3680 - accuracy: 0.8583 - val_loss: 0.3730 - val_accuracy: 0.8523

Epoch 00007: val_loss improved from 0.37533 to 0.37302, saving model to /home/samhuang/ML/best_model/best_model_binary-WpWm_CNN_kappa0.15_E/
Epoch 8/500
375/375 [==============================] - 26s 69ms/step - loss: 0.3654 - accuracy: 0.8582 - val_loss: 0.3704 - val_accuracy: 0.8534

Epoch 00008: val_loss improved from 0.37302 to 0.37042, saving model to /home/samhuang/ML/best_model/best_model_binary-WpWm_CNN_kappa0.15_E/
Epoch 9/500
375/375 [==============================] - 26s 69ms/step - loss: 0.3624 - accuracy: 0.8590 - val_loss: 0.3668 - val_accuracy: 0.8544

Epoch 00009: val_loss improved from 0.37042 to 0.36685, saving model to /home/samhuang/ML/best_model/best_model_binary-WpWm_CNN_kappa0.15_E/
Epoch 10/500
375/375 [==============================] - 26s 69ms/step - loss: 0.3608 - accuracy: 0.8594 - val_loss: 0.3666 - val_accuracy: 0.8539

Epoch 00010: val_loss improved from 0.36685 to 0.36665, saving model to /home/samhuang/ML/best_model/best_model_binary-WpWm_CNN_kappa0.15_E/
Epoch 11/500
375/375 [==============================] - 26s 69ms/step - loss: 0.3587 - accuracy: 0.8601 - val_loss: 0.3657 - val_accuracy: 0.8538

Epoch 00011: val_loss improved from 0.36665 to 0.36573, saving model to /home/samhuang/ML/best_model/best_model_binary-WpWm_CNN_kappa0.15_E/
Epoch 12/500
375/375 [==============================] - 26s 69ms/step - loss: 0.3570 - accuracy: 0.8606 - val_loss: 0.3626 - val_accuracy: 0.8548

Epoch 00012: val_loss improved from 0.36573 to 0.36262, saving model to /home/samhuang/ML/best_model/best_model_binary-WpWm_CNN_kappa0.15_E/
Epoch 13/500
375/375 [==============================] - 26s 69ms/step - loss: 0.3550 - accuracy: 0.8612 - val_loss: 0.3619 - val_accuracy: 0.8557

Epoch 00013: val_loss improved from 0.36262 to 0.36186, saving model to /home/samhuang/ML/best_model/best_model_binary-WpWm_CNN_kappa0.15_E/
Epoch 14/500
375/375 [==============================] - 26s 69ms/step - loss: 0.3533 - accuracy: 0.8614 - val_loss: 0.3644 - val_accuracy: 0.8530

Epoch 00014: val_loss did not improve from 0.36186
Epoch 15/500
375/375 [==============================] - 26s 69ms/step - loss: 0.3514 - accuracy: 0.8616 - val_loss: 0.3624 - val_accuracy: 0.8543

Epoch 00015: val_loss did not improve from 0.36186
Epoch 16/500
375/375 [==============================] - 26s 69ms/step - loss: 0.3497 - accuracy: 0.8630 - val_loss: 0.3632 - val_accuracy: 0.8540

Epoch 00016: val_loss did not improve from 0.36186
Epoch 17/500
375/375 [==============================] - 26s 69ms/step - loss: 0.3479 - accuracy: 0.8631 - val_loss: 0.3614 - val_accuracy: 0.8550

Epoch 00017: val_loss improved from 0.36186 to 0.36140, saving model to /home/samhuang/ML/best_model/best_model_binary-WpWm_CNN_kappa0.15_E/
Epoch 18/500
375/375 [==============================] - 26s 69ms/step - loss: 0.3463 - accuracy: 0.8644 - val_loss: 0.3621 - val_accuracy: 0.8545

Epoch 00018: val_loss did not improve from 0.36140
Epoch 19/500
375/375 [==============================] - 26s 70ms/step - loss: 0.3444 - accuracy: 0.8655 - val_loss: 0.3603 - val_accuracy: 0.8560

Epoch 00019: val_loss improved from 0.36140 to 0.36034, saving model to /home/samhuang/ML/best_model/best_model_binary-WpWm_CNN_kappa0.15_E/
Epoch 20/500
375/375 [==============================] - 48s 127ms/step - loss: 0.3424 - accuracy: 0.8661 - val_loss: 0.3637 - val_accuracy: 0.8547

Epoch 00020: val_loss did not improve from 0.36034
Epoch 21/500
375/375 [==============================] - 26s 69ms/step - loss: 0.3409 - accuracy: 0.8674 - val_loss: 0.3639 - val_accuracy: 0.8549

Epoch 00021: val_loss did not improve from 0.36034
Epoch 22/500
375/375 [==============================] - 26s 69ms/step - loss: 0.3373 - accuracy: 0.8707 - val_loss: 0.3642 - val_accuracy: 0.8559

Epoch 00022: val_loss did not improve from 0.36034
Epoch 23/500
375/375 [==============================] - 27s 71ms/step - loss: 0.3345 - accuracy: 0.8727 - val_loss: 0.3683 - val_accuracy: 0.8554

Epoch 00023: val_loss did not improve from 0.36034
Epoch 24/500
375/375 [==============================] - 26s 68ms/step - loss: 0.3299 - accuracy: 0.8766 - val_loss: 0.3714 - val_accuracy: 0.8549

Epoch 00024: val_loss did not improve from 0.36034
Epoch 25/500
375/375 [==============================] - 26s 70ms/step - loss: 0.3253 - accuracy: 0.8807 - val_loss: 0.3764 - val_accuracy: 0.8537

Epoch 00025: val_loss did not improve from 0.36034
Epoch 26/500
375/375 [==============================] - 26s 69ms/step - loss: 0.3186 - accuracy: 0.8859 - val_loss: 0.3827 - val_accuracy: 0.8518

Epoch 00026: val_loss did not improve from 0.36034
Epoch 27/500
375/375 [==============================] - 26s 70ms/step - loss: 0.3113 - accuracy: 0.8910 - val_loss: 0.3924 - val_accuracy: 0.8508

Epoch 00027: val_loss did not improve from 0.36034
Epoch 28/500
375/375 [==============================] - 26s 68ms/step - loss: 0.3020 - accuracy: 0.8980 - val_loss: 0.4084 - val_accuracy: 0.8492

Epoch 00028: val_loss did not improve from 0.36034
Epoch 29/500
375/375 [==============================] - 26s 69ms/step - loss: 0.2919 - accuracy: 0.9056 - val_loss: 0.4199 - val_accuracy: 0.8460

Epoch 00029: val_loss did not improve from 0.36034
Epoch 30/500
375/375 [==============================] - 26s 68ms/step - loss: 0.2820 - accuracy: 0.9122 - val_loss: 0.4272 - val_accuracy: 0.8452

Epoch 00030: val_loss did not improve from 0.36034
Epoch 31/500
375/375 [==============================] - 26s 69ms/step - loss: 0.2785 - accuracy: 0.9137 - val_loss: 0.4527 - val_accuracy: 0.8411

Epoch 00031: val_loss did not improve from 0.36034
Epoch 32/500
375/375 [==============================] - 26s 69ms/step - loss: 0.2697 - accuracy: 0.9190 - val_loss: 0.4740 - val_accuracy: 0.8349

Epoch 00032: val_loss did not improve from 0.36034
Epoch 33/500
375/375 [==============================] - 26s 70ms/step - loss: 0.2655 - accuracy: 0.9205 - val_loss: 0.5069 - val_accuracy: 0.8280

Epoch 00033: val_loss did not improve from 0.36034
Epoch 34/500
375/375 [==============================] - 26s 69ms/step - loss: 0.2512 - accuracy: 0.9280 - val_loss: 0.5443 - val_accuracy: 0.8202

Epoch 00034: val_loss did not improve from 0.36034
Epoch 00034: early stopping
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
[92mModel: "CNN"[0m
[92m_________________________________________________________________[0m
[92mLayer (type)                 Output Shape              Param #   [0m
[92m=================================================================[0m
[92msequential (Sequential)      (None, 512)               12127912  [0m
[92m_________________________________________________________________[0m
[92mdense_2 (Dense)              multiple                  1026      [0m
[92m=================================================================[0m
[92mTotal params: 12,128,938[0m
[92mTrainable params: 12,128,934[0m
[92mNon-trainable params: 4[0m
[92m_________________________________________________________________[0m
None

@LAYER1       @@@@@@@@@@@@@@@@@@@@@@
[94mModel: "sequential"[0m
[94m_________________________________________________________________[0m
[94mLayer (type)                 Output Shape              Param #   [0m
[94m=================================================================[0m
[94mlambda (Lambda)              (None, 75, 75, 2)         0         [0m
[94m_________________________________________________________________[0m
[94mbatch_normalization (BatchNo (None, 75, 75, 2)         8         [0m
[94m_________________________________________________________________[0m
[94mconv2d (Conv2D)              (None, 75, 75, 32)        2336      [0m
[94m_________________________________________________________________[0m
[94mmax_pooling2d (MaxPooling2D) (None, 37, 37, 32)        0         [0m
[94m_________________________________________________________________[0m
[94mconv2d_1 (Conv2D)            (None, 37, 37, 128)       65664     [0m
[94m_________________________________________________________________[0m
[94mmax_pooling2d_1 (MaxPooling2 (None, 18, 18, 128)       0         [0m
[94m_________________________________________________________________[0m
[94mconv2d_2 (Conv2D)            (None, 18, 18, 256)       1179904   [0m
[94m_________________________________________________________________[0m
[94mmax_pooling2d_2 (MaxPooling2 (None, 9, 9, 256)         0         [0m
[94m_________________________________________________________________[0m
[94mdropout (Dropout)            (None, 9, 9, 256)         0         [0m
[94m_________________________________________________________________[0m
[94mflatten (Flatten)            (None, 20736)             0         [0m
[94m_________________________________________________________________[0m
[94mdense (Dense)                (None, 512)               10617344  [0m
[94m_________________________________________________________________[0m
[94mdropout_1 (Dropout)          (None, 512)               0         [0m
[94m_________________________________________________________________[0m
[94mdense_1 (Dense)              (None, 512)               262656    [0m
[94m_________________________________________________________________[0m
[94mdropout_2 (Dropout)          (None, 512)               0         [0m
[94m=================================================================[0m
[94mTotal params: 12,127,912[0m
[94mTrainable params: 12,127,908[0m
[94mNon-trainable params: 4[0m
[94m_________________________________________________________________[0m
None
[92m%Optimizer:
[0m {'name': 'Adam', 'learning_rate': 1e-04, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}
[92m%Layer detail:
[0m {'name': 'sequential', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 75, 75, 2), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'lambda_input'}}, {'class_name': 'Lambda', 'config': {'name': 'lambda', 'trainable': True, 'dtype': 'float32', 'function': ('4wEAAAAAAAAAAAAAAAEAAAAGAAAAUwAAAHMgAAAAfABkAGQAhQJkAGQAhQJkAGQAhQJkAGQAhQJm\nBBkAUwApAU6pAKkB2gF4cgEAAAByAQAAAPofL2hvbWUvc2FtaHVhbmcvTUwvQ05OL21vZGVscy5w\nedoIPGxhbWJkYT4NAAAA8wAAAAA=\n', None, None), 'function_type': 'lambda', 'module': 'models', 'output_shape': None, 'output_shape_type': 'raw', 'output_shape_module': None, 'arguments': {}}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d', 'trainable': True, 'dtype': 'float32', 'filters': 32, 'kernel_size': (6, 6), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'MaxPooling2D', 'config': {'name': 'max_pooling2d', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_1', 'trainable': True, 'dtype': 'float32', 'filters': 128, 'kernel_size': (4, 4), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'MaxPooling2D', 'config': {'name': 'max_pooling2d_1', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_2', 'trainable': True, 'dtype': 'float32', 'filters': 256, 'kernel_size': (6, 6), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'MaxPooling2D', 'config': {'name': 'max_pooling2d_2', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Dropout', 'config': {'name': 'dropout', 'trainable': True, 'dtype': 'float32', 'rate': 0.1, 'noise_shape': None, 'seed': None}}, {'class_name': 'Flatten', 'config': {'name': 'flatten', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense', 'trainable': True, 'dtype': 'float32', 'units': 512, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_1', 'trainable': True, 'dtype': 'float32', 'rate': 0.5, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 512, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_2', 'trainable': True, 'dtype': 'float32', 'rate': 0.5, 'noise_shape': None, 'seed': None}}]}

@LAYER2       @@@@@@@@@@@@@@@@@@@@@@
[92m%Layer detail:
[0m {'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 2, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}

****************************************************
history keys:
 dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])
history params:
 {'verbose': 1, 'epochs': 500, 'steps': None}
****************************************************
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
The data set contains images
60000/60000 [==============================] - 261s 4ms/step - loss: 0.3608 - accuracy: 0.8559
Testing Loss = 0.360772, Testing Accuracy = 0.855950
The data set contains images
[[0.7162452936172485, 0.28375473618507385], [0.9937711358070374, 0.006228879559785128], [0.9128145575523376, 0.08718543499708176], [0.011600981466472149, 0.9883990287780762], [0.3760153651237488, 0.6239845752716064], [0.9907060265541077, 0.00929398275911808], [0.11259032040834427, 0.8874096274375916], [0.7233139276504517, 0.27668607234954834], [0.7794532179832458, 0.22054681181907654], [0.07426747679710388, 0.9257325530052185]]
[[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
2
$W^+$ (auc = 0.9337)
$W^-$ (auc = 0.9337)
