

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
New Run
Current date and time = 2023-01-31 15:21:36.980838
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
CNN2_torch(
  (h2ptjl): Sequential(
    (0): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): Conv2d(2, 32, kernel_size=(6, 6), stride=(1, 1), padding=same)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Sequential(
      (0): ResidualBlock(
        (left): Sequential(
          (0): Conv2d(32, 128, kernel_size=(6, 6), stride=(1, 1), padding=same)
          (1): ReLU(inplace=True)
          (2): Conv2d(128, 128, kernel_size=(6, 6), stride=(1, 1), padding=same)
        )
        (shortcut): Sequential(
          (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), padding=same)
        )
      )
      (1): ResidualBlock(
        (left): Sequential(
          (0): Conv2d(128, 128, kernel_size=(6, 6), stride=(1, 1), padding=same)
          (1): ReLU(inplace=True)
          (2): Conv2d(128, 128, kernel_size=(6, 6), stride=(1, 1), padding=same)
        )
        (shortcut): Sequential()
      )
    )
    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Sequential(
      (0): ResidualBlock(
        (left): Sequential(
          (0): Conv2d(128, 256, kernel_size=(6, 6), stride=(1, 1), padding=same)
          (1): ReLU(inplace=True)
          (2): Conv2d(256, 256, kernel_size=(6, 6), stride=(1, 1), padding=same)
        )
        (shortcut): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), padding=same)
        )
      )
      (1): ResidualBlock(
        (left): Sequential(
          (0): Conv2d(256, 256, kernel_size=(6, 6), stride=(1, 1), padding=same)
          (1): ReLU(inplace=True)
          (2): Conv2d(256, 256, kernel_size=(6, 6), stride=(1, 1), padding=same)
        )
        (shortcut): Sequential()
      )
    )
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Dropout(p=0.1, inplace=False)
  )
  (linear1): Linear(in_features=20736, out_features=512, bias=True)
  (linear2): Linear(in_features=512, out_features=32, bias=True)
  (linear3): Linear(in_features=512, out_features=128, bias=True)
  (linear4): Linear(in_features=128, out_features=32, bias=True)
  (_output): Linear(in_features=32, out_features=3, bias=True)
)
Send the model to cuda
VALIDATION:   kappa = 1.05000     | val loss= 0.980, val acc= 47.05% |
Epoch 1: val_loss improved from inf to 0.9797, saving model to/home/samhuang/ML/best_model/best_model_ternary2_P-CNN2_event_kappa0.23_fiximag/Try/0
VALIDATION:   kappa = 1.05000     | val loss= 0.951, val acc= 49.55% |
Epoch 2: val_loss improved from 0.9797 to 0.9512, saving model to/home/samhuang/ML/best_model/best_model_ternary2_P-CNN2_event_kappa0.23_fiximag/Try/0
VALIDATION:   kappa = 1.05000     | val loss= 0.948, val acc= 50.70% |
Epoch 3: val_loss improved from 0.9512 to 0.9475, saving model to/home/samhuang/ML/best_model/best_model_ternary2_P-CNN2_event_kappa0.23_fiximag/Try/0
VALIDATION:   kappa = 1.05000     | val loss= 0.938, val acc= 52.08% |
Epoch 4: val_loss improved from 0.9475 to 0.9383, saving model to/home/samhuang/ML/best_model/best_model_ternary2_P-CNN2_event_kappa0.23_fiximag/Try/0
VALIDATION:   kappa = 1.05000     | val loss= 0.914, val acc= 52.10% |
Epoch 5: val_loss improved from 0.9383 to 0.9144, saving model to/home/samhuang/ML/best_model/best_model_ternary2_P-CNN2_event_kappa0.23_fiximag/Try/0
VALIDATION:   kappa = 1.05000     | val loss= 0.941, val acc= 53.87% |
Epoch   6: val_loss did not improve from 0.9144. Performance did not improve for  1 epoch(s)
VALIDATION:   kappa = 1.05000     | val loss= 0.864, val acc= 53.11% |
Epoch 7: val_loss improved from 0.9144 to 0.8639, saving model to/home/samhuang/ML/best_model/best_model_ternary2_P-CNN2_event_kappa0.23_fiximag/Try/0
VALIDATION:   kappa = 1.05000     | val loss= 1.015, val acc= 51.61% |
Epoch   8: val_loss did not improve from 0.8639. Performance did not improve for  1 epoch(s)
VALIDATION:   kappa = 1.05000     | val loss= 0.990, val acc= 51.91% |
Epoch   9: val_loss did not improve from 0.8639. Performance did not improve for  2 epoch(s)
VALIDATION:   kappa = 1.05000     | val loss= 1.387, val acc= 50.72% |
Epoch  10: val_loss did not improve from 0.8639. Performance did not improve for  3 epoch(s)
VALIDATION:   kappa = 1.05000     | val loss= 1.557, val acc= 49.50% |
Epoch  11: val_loss did not improve from 0.8639. Performance did not improve for  4 epoch(s)
VALIDATION:   kappa = 1.05000     | val loss= 2.058, val acc= 49.85% |
Epoch  12: val_loss did not improve from 0.8639. Performance did not improve for  5 epoch(s)
VALIDATION:   kappa = 1.05000     | val loss= 1.998, val acc= 48.89% |
Epoch  13: val_loss did not improve from 0.8639. Performance did not improve for  6 epoch(s)
VALIDATION:   kappa = 1.05000     | val loss= 2.049, val acc= 49.90% |
Epoch  14: val_loss did not improve from 0.8639. Performance did not improve for  7 epoch(s)
VALIDATION:   kappa = 1.05000     | val loss= 2.470, val acc= 48.08% |
Epoch  15: val_loss did not improve from 0.8639. Performance did not improve for  8 epoch(s)
VALIDATION:   kappa = 1.05000     | val loss= 2.338, val acc= 48.93% |
Epoch  16: val_loss did not improve from 0.8639. Performance did not improve for  9 epoch(s)
VALIDATION:   kappa = 1.05000     | val loss= 2.514, val acc= 49.24% |
Epoch  17: val_loss did not improve from 0.8639. Performance did not improve for 10 epoch(s)
dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])
TEST:   kappa = 1.05000     | test loss= 1.474, test acc= 52.99% |
Finished Training
N of classes 3
$W^+/W^-$ (auc = 65.82 +- 0.0000 %)
$W^+/Z$ (auc = 72.45 +- 0.0000 %)
$W^-/Z$ (auc = 72.86 +- 0.0000 %)
N of classes 3
$W^+/W^-$ (acc = 49.86 +- 0.0000 %
$W^+/Z$ (acc = 53.70 +- 0.0000 %
$W^-/Z$ (acc = 55.18 +- 0.0000 %
The summarized testing accuracy = 52.99 +- 0.0000 %, with the loss = 1.4737 +- 0.000000
Finished Training
